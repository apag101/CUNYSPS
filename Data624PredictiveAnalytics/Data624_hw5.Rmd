---
title: "Data 624 Homework 5"
author: "Anthony Pagan"
date: "3/7/2020"
output: 
    html_document:
        toc: true
        toc_float: true
        toc_depth: 5
        css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
#knitr::opts_chunk$set(echo = TRUE)
require(knitr)
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
library(gridExtra)
library(reshape2)
library(leaps)
library(pROC)
library(caret)
library(naniar)
library(pander)
library(pROC)
library(mlbench)
library(e1071)
library(fpp2)
```

## 1

Consider the pigs series — the number of pigs slaughtered in Victoria each
month.

### (a) 

Use the ses() function in R to and the optimal values of @ and lo , and
generate forecasts for the next four months.

```{r}
data(pigs)
ses(pigs)
pc<-ses(pigs, h=4)
autoplot(pc)+
    autolayer(fitted(pc), series="Fitted")
```

### (b)

Compute a 95% prediction interval for the first forecast using y ± 1.96s
where is the standard deviation of the residuals. Compare your interval
with the interval produced by R.

```{r}
pcsd<-sqrt(var(residuals(pc)))

p<-head(pc[["mean"]],1)
c(Lower95 = p - 1.96 * pcsd,
  Upper95 = p + 1.96 * pcsd)

pcsd <- sqrt(sum(residuals(pc)^2)/(length(pigs)-2))
c(Lower95 = p - 1.96 *  pcsd,
  Upper95 = p+ 1.96 *  pcsd)

```

## 5

Data set books contains the daily sales of paperback and hardcover books at
the same store. The task is to forecast the next four days’ sales for
paperback and hardcover books.


### (a)

Plot the series and discuss the main features of the data.

```{r}
data(books,)
autoplot(books)
```

### (b)

Use the ses() function to forecast each series, and plot the forecasts.

```{r}
pb<-c(books[,'Paperback'])
hc<-c(books[,'Hardcover'])

bkspb<-ses(pb,h=4)
bkshc<-ses(hc,h=4)

autoplot(books)+
    autolayer(bkspb, series="Paperback", PI=FALSE)+
    autolayer(bkshc, series="Hardcover", PI=FALSE)
```

### (c)

Compute the RMSE values for the training data in each case.

```{r}
accuracy(bkspb)
accuracy(bkshc)
```


## 6 

### (a)

Now apply Holt’s linear method to the paperback and hardback series
and compute four-day forecasts in each case.

```{r}
pba<-holt(pb, h=4)
hca<-holt(hc,h=4)

autoplot(books)+
    autolayer(pba, series="Paperback", PI=FALSE)+
    autolayer(hca, series="Hardcover", PI=FALSE)

```

### (b)

Compare the RMSE measures of Holt’s method for the two series to
those of simple exponential smoothing in the previous question.
(Remember that Holt’s method is using one more parameter than SES.)
Discuss the merits of the two forecasting methods for these data sets.


```{r}
accuracy(bkspb)
accuracy(bkshc)
accuracy(pba)
accuracy(hca)

```

### (c)

Compare the forecasts for the two series using both methods. Which do
you think is best?

**The simple exponential smoothing has a higher RMSE value compare to the holts method. The books data has a clear trend and the holts method was specifically built to expend simple exponential smoothing to allow forecasting for data with trends. **


### (d)

Calculate a 95% prediction interval for the first forecast for each series,
using the RMSE values and assuming normal errors. Compare your
intervals with those produced using ses and holt .

```{r}
pbasd<-sqrt(pba$model$mse)

c(Lower95 = pba$mean[1] - 1.96 * pbasd,
  Upper95 = pba$mean[1] + 1.96 * pbasd)

hcasd<-sqrt(hca$model$mse)

c(Lower95 = pba$mean[1] - 1.96 * hcasd,
  Upper95 = pba$mean[1] + 1.96 * hcasd)

```

## 7

For this exercise use data set eggs , the price of a dozen eggs in the United
States from 1900–1993. Experiment with the various options in the holt()
function to see how much the forecasts change with damped trend, or with a
Box-Cox transformation. Try to develop an intuition of what each argument
is doing to the forecasts.
[Hint: use h=100 when calling holt() so you can clearly see the
differences between the various options when plotting the forecasts.]
Which model gives the best RMSE?

**The holt method with damped option has a lower RSME and therefore is the best model.**


```{r}
autoplot(eggs)

ht100<-holt(eggs, h=100)
ht100d<-holt(eggs, h=100, damped = TRUE)
autoplot(ht100)+
    autolayer(ht100, series = "Holts Method",  PI=FALSE)+
    autolayer(ht100d, series = "Damped Holts method" , PI=FALSE)+
    ggtitle("Eggs")+ xlab("Year")+
    ylab("Eggs")+
    guides(colour=guide_legend(title="Forecast"))

lambda<-BoxCox.lambda(eggs)
egg100<-rwf(eggs,h=100, lambda=lambda)

autoplot(BoxCox(eggs, lambda))+
    autolayer(egg100, series="egg100")

accuracy(ht100d)
accuracy(ht100)
accuracy(egg100)
```

## 8 

Recall your retail time series data (from Exercise 3 in Section 2.10).

### (a)

Why is multiplicative seasonality necessary for this series?

**The increasing trend tells us that the multiplicative series is necessary.**

```{r} 
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349873A"],
  frequency=12, start=c(1982,4))
checkresiduals(myts)
myts %>% decompose(type="multiplicative")%>%
  autoplot(frequency = frequency(retaildata)) + xlab("Year") +
  ggtitle("Classical multiplicative decomposition of retail datas")

```

### (b)

Apply Holt-Winters’ multiplicative method to the data. Experiment with
making the trend damped.

```{r}
fit1 <- hw(myts,seasonal="additive")
fit2 <- hw(myts,seasonal="multiplicative")
autoplot(myts) +
  autolayer(fit1, series="HW additive forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Retail Sales") +
  ggtitle("Retail Data") +
  guides(colour=guide_legend(title="Forecast"))

fit3 <- hw(myts,seasonal="additive", damped = TRUE)
fit4 <- hw(myts,seasonal="multiplicative", damped = TRUE)
autoplot(myts) +
  autolayer(fit3, series="HW additive forecasts", PI=FALSE) +
  autolayer(fit4, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("Retail Sales") +
  ggtitle("Retail Data Damped") +
  guides(colour=guide_legend(title="Forecast Damped"))
```


### (c)

Compare the RMSE of the one-step forecasts from the two methods.
Which do you prefer?

**The lower RMSE of the multiplicative method tells us that it would be the prefered method. In fact , all multiplicative accuracy values are smaller and are preferred.**

```{r}
accuracy(fit1,h=1)
accuracy(fit2, h=1)
accuracy(fit3, h=1)
accuracy(fit4, h=1)
```

### (d)

Check that the residuals from the best method look like white noise.

**The residuals for the multiplicative method border are probably not white noise as 2 bars are past the boundaries and 2 more touching or are barely past the boundaries.**

```{r}
ggAcf(residuals(fit2))
```

### (e)

Now end the test set RMSE, while training the model to the end of 2010.
Can you beat the seasonal naïve approach from Exercise 8 in Section 3.7?

```{r}
myts2 <- ts(retaildata[,"A3349873A"],
  frequency=12, start=c(1982,4), end=c(2010,4))


emyts2<-ets(myts2, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL,
    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE,
    additive.only=FALSE, restrict=TRUE,
    allow.multiplicative.trend=FALSE)



emyts2 %>% forecast(h=24) %>%
  autoplot() +
  ylab("Retail Sales")

fit2 <- hw(myts2,seasonal="multiplicative")
autoplot(fit2)

cbind('Residuals' = residuals(emyts2), 'Forecast errors' =residuals(emyts2, type='response')) %>%
    autoplot(facet=TRUE)+ xlab("Year")+ylab("")

accuracy(emyts2)
accuracy(fit2)
accuracy(snaive(myts2))

summary(emyts2)
```

## 9

For the same retail data, try an STL decomposition applied to the Box-Cox
transformed series, followed by ETS on the seasonally adjusted data. How
does that compare with your best previous forecasts on the test set?

**The ETS on the BoxCox transformed data has a lower RMSE and is better forecast**

```{r}
lambda<-BoxCox.lambda(myts2)
bmyts<-BoxCox(myts2, lambda)

ebmyts<-ets(myts2, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL,
    gamma=NULL, phi=NULL, lambda=lambda, biasadj=FALSE,
    additive.only=FALSE, restrict=TRUE,
    allow.multiplicative.trend=FALSE)


ebmyts %>% forecast(h=24) %>%
  autoplot() +
  ylab("Retail Sales")

accuracy(emyts2)
accuracy(fit2)
accuracy(ebmyts)

summary(ebmyts)
```

## APPENDIX

**Code used in analysis**
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```

