---
title: "Data 612 Project"
author: "Anthony Pagan"
date: "6/7/2020"
output: 
    html_document:
        toc: true
        toc_float: true
        toc_depth: 5
        css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
#knitr::opts_chunk$set(echo = TRUE)
require(knitr)
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
library(gridExtra)
library(reshape2)
library(leaps)
library(pROC)
library(caret)
library(naniar)
library(pander)
library(pROC)
library(mlbench)
library(e1071)
library(fpp2)
library(mlr)
library(recommenderlab)
```

```{r}
#Chapter 4
data("MovieLense")
class(MovieLense)
#as.vector(MovieLense@data)
ratings_Movies<-MovieLense[rowCounts(MovieLense)>50, colCounts(MovieLense)>100]
ratings_Movies

percentage_training<-.8
min(rowCounts(ratings_Movies))

items_to_keep<- 15
rating_thresold<- 3
n_eval<-1

eval_sets <- evaluationScheme(data=ratings_Movies, method="split", train=percentage_training, given= items_to_keep, goodRating = rating_thresold, k=n_eval)
eval_sets

getData(eval_sets, "train")
nrow(getData(eval_sets, "train")) /nrow(ratings_Movies)
getData(eval_sets, "known")
getData(eval_sets, "unknown")
nrow(getData(eval_sets, "known")) /nrow(ratings_Movies)
unique(rowCounts(getData(eval_sets, "known")))

qplot(rowCounts(getData(eval_sets, "unknown"))) + geom_histogram(binwidth = 10) + ggtitle("unknown items by users")

#Bootstrapping
eval_sets2 <- evaluationScheme(data=ratings_Movies, method="bootstrap", train=percentage_training, given= items_to_keep, goodRating = rating_thresold, k=n_eval)
eval_sets2
per_test<-nrow(getData(eval_sets2, "known")) /nrow(ratings_Movies)
per_test

per_train<-length(unique(eval_sets2@runsTrain[[1]]))/nrow(ratings_Movies)
per_test+per_train

table_train<-table(eval_sets2@runsTrain[[1]])
n_repetitions<-factor(as.vector(table_train))
qplot(n_repetitions)+ggtitle("Number of repetitiions in the training set")

#Cross Validation K-fold 

n_fold<-4
eval_sets3<-evaluationScheme(data=ratings_Movies, method="cross-validation", k=n_fold, given= items_to_keep, goodRating = rating_thresold)
size_sets<-sapply(eval_sets3@runsTrain, length)
size_sets

#Evaluation Techniques
model_to_evaluate<- "IBCF"
model_parameters<-NULL

eval_recommender <- Recommender(data=getData(eval_sets3, "train"), method=model_to_evaluate, parameter=model_parameters)
items_to_recommend<-10

eval_prediction<-predict(object= eval_recommender, newdata=getData(eval_sets3, "known"), n=items_to_recommend, type="ratings")
class(eval_prediction)

qplot(rowCounts(eval_prediction)) + geom_histogram(binwidth = 10) + ggtitle("Distribution of movies per user")

eval_accuracy<- calcPredictionAccuracy(x = eval_prediction, data=getData(eval_sets3, "unknown"), byUser=TRUE)
head(eval_accuracy)

qplot(eval_accuracy[,"RMSE"]) + geom_histogram(binwidth = .1) + ggtitle("Distribution of the RMSE by user")

eval_accuracy2<- calcPredictionAccuracy(x = eval_prediction, data=getData(eval_sets3, "unknown"), byUser=FALSE)
eval_accuracy2

#Evaluate Recommendations
results<-evaluate(x= eval_sets3, method= model_to_evaluate, n=seq(10,100,10))
class(results)
head(getConfusionMatrix(results)[[1]])
columns_to_sum<-c("TP","FP","FN","TN")
indices_summed<-Reduce("+",getConfusionMatrix(results))[,columns_to_sum]
head(indices_summed)
plot(results, annotate= TRUE, main= "ROC Curve")
plot(results, "prec/rec", annotate=TRUE, main="Precision-recall")

models_to_evaluate<- list(
    IBCF_cos = list(name="IBCF", param=list(method="cosine")),
    IBCF_cor = list(name="IBCF", param=list(method="pearson")),
    UBCF_cos = list(name="UBCF", param=list(method="cosine")),
    IBCF_cor = list(name="IBCF", param=list(method="pearson")),
    IBCF_cos = list(name="RANDOM", param=NULL))
n_recommendations<-c(1,5,seq(10,100,10))

list_results<-evaluate(x=eval_sets3, method=models_to_evaluate, n=n_recommendations)
class(list_results[[1]])
sapply(list_results, class)== "evaluationResults"
avg_matrices<-lapply(list_results,avg)
head(avg_matrices$IBCF_cos[,5:8])

#SELEC MODEL
plot(list_results, annotate=1, legend="topleft") 
title("ROC curve")
plot(list_results,"prec/rec", annotate=1, legend="bottomright") 
title("Precision-recall")

#Select best parameter for model
vector_k <- c(5,10,20,30,40)
models_to_evaluate <- lapply(vector_k, function(k){
    list(name="IBCF", param=list(method="cosine", k=k))
})
names(models_to_evaluate)<-paste0("IBCF_k_", vector_k)
list_results<-evaluate(x=eval_sets3, method=models_to_evaluate, n=n_recommendations)
plot(list_results, annotate=1, legend="topleft") 
title("ROC curve")
plot(list_results,"prec/rec", annotate=1, legend="bottomright") 
title("Precision-recall")

```

```{r}


```

## APPENDIX

**Code used in analysis**
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```

