---
title: "R Notebook"
output: html_notebook
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)

rm(list=ls())
list.of.packages <- c("alluvial","caret","caret","corrplot","corrplot","data.table","dplyr","faraway","forcats","geosphere","ggplot2","ggplot2","ggplot2","grid","gridExtra","jtools","kableExtra","knitr","leaflet","leaflet.extras","leaps","lubridate","maps","MASS","mice","naniar","pander","patchwork","prettydoc","pROC","psych","RColorBrewer","readr","reshape2","scales","stringr","tibble","tidyr","tidyverse","xgboost","widgetframe","Rcpp","mlbench","fpp2","mlr","jsonlite","devtools","sparklyr","SparkR","readtext","magrittr","simmer","quanteda","tidytext","tm","SnowballC","text2vec","purrr","topicmodels","RMySQL","plotly","GGally","corrplot","imputeTS","keras","tensorflow","reticulate")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dependencies=TRUE)

#memory.limit(size=60000)
#install_tensorflow(gpu=TRUE)
#devtools::install_github("rstudio/tensorflow")
#devtools::install_github("rstudio/keras")
#devtools::install_github("rstudio/reticulate")
#knitr::opts_chunk$set(echo = TRUE)
require(knitr)
library(ggplot2)
library(tidyr)
library(MASS)
library(psych)
library(kableExtra)
library(dplyr)
library(faraway)
library(gridExtra)
library(reshape2)
library(leaps)
library(pROC)
library(caret)
library(pROC)
library(mlbench)
library(e1071)
library(fpp2)
library(mlr)
library(recommenderlab)
library(jsonlite)
library(stringr)
library(devtools)
library(sparklyr)
library(readtext)
library(simmer)
library(quanteda)
library(tidytext)
library(tm)
library(purrr)
library(topicmodels)
library(RMySQL)
library(plotly)
library(GGally)
library(corrplot)
library(imputeTS)
library(car)
library(rpart)
library(keras)
library(tensorflow)
library(naniar)
library(reticulate)

```


```{r message=FALSE, warning=FALSE}
setwd(getwd())
d1<- as.data.frame(read.csv('TestQuery.csv', header = TRUE,quote = ""))
d1<-rename(d1, DateTime=names(d1[1]))

cnames<-c('AlertName','AlertDescription','Severity','Priority','DateTime','ManagedEntityGuid')
d2<- as.data.frame(read.csv('TestQuery3.csv',header= FALSE, col.names = cnames, quote = "\"", sep=","))

joineddata<-right_join(d1, d2,  by = c("DateTime","ManagedEntityGuid"))
joineddata<-subset(joineddata,!is.na(SampleValue))%>%
  subset(select=c(-AlertName.x,-Severity.x))%>%
  rename(AlertName = AlertName.y, Severity = Severity.y)%>%
  mutate(ServerName = case_when
         ((ManagedEntityGuid == '2A909362-BE1E-F148-835C-96787AE2775E') ~ 'Server1',
         (ManagedEntityGuid == '6EA459BD-0616-71F6-FC4A-7989A2DCF9FD') ~ 'Server2',
         (ManagedEntityGuid == 'A1A8A538-8B75-B8F5-5F44-7159F192F602') ~ 'Server3',
         (ManagedEntityGuid == 'AF487CE3-8C62-2C7A-F748-9241D73F0403') ~ 'Server4',
          TRUE ~ 'Server5'))%>%
  mutate(Ticket = case_when
         ((Severity == 2) ~ 1,
           TRUE ~ 0))

sdata<-joineddata%>%subset(select=c(-Objectname, -AlertDescription))%>%
  group_by(DateTime, ServerName, CounterName, AlertName)%>%
  mutate(row_id=1:n()) %>% ungroup() %>%
  spread(CounterName, SampleValue)%>%
  subset(select=c(-row_id,-Priority))

nonats<-na_kalman(sdata)

#Remove highly correllated Ops master* column, replace with avg
df <- nonats %>% select(starts_with("Op Master"))
#df<-names(df)
nonats<-nonats%>%mutate(MasterAvg=rowMeans(cbind(nonats[names(df)])))
nonats<-nonats%>%select(!c(names(df)))
#remove any columns with NA
nonats2<-nonats%>%
  select(where(~!any(is.na(.))))
#replace NA with 0
nonats3<-nonats%>%na.replace()

nn<-nonats2[c(7:ncol(nonats2))]



df<-nn# Convert the data.frame to a matrix
data <- as.matrix(df)
# Remove variable names
dimnames(data) <- NULL

# Split for train and test data
set.seed(123)
indx <- sample(2,
               nrow(data),
               replace = TRUE,
               prob = c(0.7, 0.3)) # Makes index with values 1 and 2


#The first 21 columns containing the feature variables are separated into two computer variables (according to the random index created above).

x_train <- data[indx == 1, 2:7] # Take rows with index = 1
x_test <- data[indx == 2, 2:7]


#A separate computer variable is created to hold the ground-truth (actual) feature values of the test set for later use.

#y_test_actual <- data[indx == 2, 22]
y_test_actual <- data[indx == 2, 1]

#The feature variables of the training and test sets can be one-hot-encoded using the Keras function to_categorical().

# Using similar indices to correspond to the training and test set
y_train <-to_categorical(data[indx == 1, 1])
y_test <- to_categorical(data[indx == 2, 1])

#The sample spaces of the feature variables are not scaled equally. The code chunk below calculates the mean and the standard deviation of the training set and then normalizes both the training and test sets using the mean and standard deviation of the training set (it is important to use the statistics from the training set only!).


mean_train <- apply(x_train,
                    2,
                    mean)
std_train <- apply(x_train,
                   2,
                   sd)
x_train <- scale(x_train,
                 center = mean_train,
                 scale = std_train)
x_test <- scale(x_test,
                center = mean_train,
                scale = std_train)

#Creating a model
#The model is created in the code chunk below. It comprises a simple sequential layout. Note the use of a pipeline to add new layers. The first is a densely connected layer with 22 nodes. It uses L2 regularization to combat overfitting. The rectified linear unit, relu, activation function is used for this layer. Finally the input dimension is set to the number of feature variables in the data. Deeper layers infer the dimensions from prior layers and are not specified.


#The next layer is a dropout layer also employed to combat overfitting and drops an 0.2 fraction of weights. The next layer is a densely connected layer with 12 nodes and also uses L2 regularization. The final layer comprises 3 nodes to equal the number of target classes and uses softmax activation so as to provide three probability outputs (summing to 1). The summary() function summarizes the model and shows a total of 799 trainable parameters.

# Creating the model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 7,
              kernel_regularizer = regularizer_l2(0.001),
              activation = "relu",
              input_shape = c(6)) %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 12,
              kernel_regularizer = regularizer_l2(0.001),
              activation = "relu") %>% 
  layer_dense(units = 2,
              activation = "softmax")
summary(model)


#Compiling the model Before fitting, the model requires compilation. The loss function, optimizer, and metrics are specified during this step. In this example categorical cross-entropy is used as the loss function (since this is a multi-class classification problem). A standard ADAM optimizer is used and accuracy is used as the metric.

# Compiling the model
model %>% compile(loss = "categorical_crossentropy",
                  optimizer = "adam",
                  metrics = c("accuracy"))



#Fitting the data The training set is used to fit the compiled model. In addition a validation set is created during the training and is set to comprise a fraction of 0.2 of the training data. The fitted model is saved in a computer variable named history. Twenty epochs are run, with a mini-batch size of 64. When using Keras in RStudio, two live plots are created in the Viewer tab. The top shows the loss values for the training and validation sets. The bottom plot shows the accuracy of the two sets.

history <- model %>% 
  fit(x_train,
      y_train,
      epoch = 20,
        batch_size = 64,
      validation_split = 0.2)

#A simple plot can be created to show the loss and the accuracy over the epochs.

plot(history)

model%>%
  evaluate(x_test,
           y_test)

pred<-model%>%
  predict_classes(x_test)

table(Predicted = pred,
      Actual = y_test_actual)

prob<-model%>%
  predict_proba(x_test)

results<-cbind(prob,
      pred,
      y_test_actual)

```

