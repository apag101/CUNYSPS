Homework 3 
By Anthony Pagan

1) you can  provide feedback how he can improve his approach to  data science.

Some of the suggestions to improve approach include

	• Cleanup the doors and persons columns to be more consistent, either set more to 5 or bin 1-3 doors and 4-5 doors. 
	• Use set seed so the output values will be same when  rerunning code. 
	• Use AUC/ROC plots to compare each model 
	• Output file should include the classifier used to differentiate data.

2) You may also identify and justify the choice of classifiers Ruffio ran to complete this task.

Ruffio used the 7 classifiers described below:

	• VGLM - Uses the GAM or generalized additive model that fits a linear model to each X and adds each result together. One negative is doesn't  for interaction groups natively it has to be added separately. GAM can be used for categorical values with 0 and 1 values. Since Y in this dataset has 4 possible values,  and would need the creation of dummy variables. 

	• LDA- This classifier is also used for categorical values and can take each member of a class and divide predictors accordingly finding a linear combination of data attributes that best separate the data into classes. 

	• RPART- This is a basic regression tree which uses nested if then statements for predictors leaving simple averages. This model can overfit with too many trees. They may be better off using m5tree or bagging/boosting/Random forest

	• Bagging -Uses bootstrapping to take multiple samples the averages all model prediction results. Usually used in regression trees. Although it improves predictions it is not as interpretable as other models. Trees are not pruned unlike Random Forest, that is why bagging has high variance but low bias.

	• GBM 1-3 - GBM is used for boosting. Boosting is similar to bagging except each tree is grown using information form previously grown trees. Also it does not use bootstrap and instead each tree is fit on a modified version of the original dataset. 

	• Random Forest is similar to bagging  with an added ability to of reduction number of predictors used at each level of the split.

	• XGBOOST (xgb 4 and 5)- XBoost is a similar form of boosting but with better performance for learning. 

Without looking at the performance metrics from the 2 linear classifiers VGLM and LDA , LDA would be a better choice since it divides the classes. As for the tree model classifiers  i would lean toward Random Forest for its added ability to reduce the predictors. 

3) You can compare the performance metrics of classifiers and provide an explanation for the observed performance variances.

Each classifier uses different methods of prediction, some classify groups and some average models via bootstrapping. Each methods results in different confusion matrix results. The key metrics that I am looking at in Ruffio's output is the accuracy, no information rate  and kappa. 

The accuracy gives us where the model predicted values true or false accurately. No information rate is the rate you can predict about a value that not in the class you are trying to predict for current distribution. If NIR is higher than accuracy you have a higher chance at predicting anything outside of predicted class. In the confusion matrix output,  the NIR is the same rate of ~68% . The Kappa tells us how good our predictions are versus random guessing. This is useful when we have an imbalanced dataset. Ruffio's dataset is imbalanced with unacc being 70% of the data:

acc  good unacc vgood 
  263    46   858    43 

The 2 linear classifiers GLM and LDA appears to be the clear winner for this dataset. With a high accuracy and high kappa score we can say these model have accurate prediction that is not based on the unbalanced data and is highly likely to be better than random guessing.  The Tree based classifiers have lower accuracy and Kappa values.

       Accuracy : 0.9749          
                 95% CI : (0.9575, 0.9866)
    No Information Rate : 0.6795          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9481 

               Accuracy : 0.9691          
                 95% CI : (0.9503, 0.9822)
    No Information Rate : 0.6795          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9363          
                                          
Based on these metrics I would chose VGLM over LDA.
