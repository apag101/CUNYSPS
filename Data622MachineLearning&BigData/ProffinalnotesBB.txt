Assumptions and Model Building  Collapse

Most important takeaway for this course is that modeling (for regression or classification, or for that matter any modeling) requires some assumption about the manner in which the data was collected or the underlying process that generated the data and models are fundamentally based on some assumption. To quote Box, "All Models are wrong and some are useful".

Therefore, It is  important for those who build models to ensure the assumptions  of a model are not violated. It is IMPORTANT to NOT apply a model when the assumptions are violated. Garbage In Garbage Out. We must catalog the models,their assumptions and requirements before proceeding further... We also discussed the problems of imbalanced datasets, few weeks ago. These are checks and balances before doing anything with a model. Please understand that this is about the data and such errors can be easily averted. There is no excuse for a model builder to blindly run  complex models without understanding the data they seek to analyze.

Let us now turn to the domain of Logistic Regression (which predates Classification). Here the domain of Y dependent variable in a binary classification problem is dichotomous -- taking one of two values. When a variable is either 0 or 1 that variable is not normally distributed. While this is obvious we have to recognize that it is not why linear model is not suitable. variable does NOT have to be normally distributed. Again, it is not necessary for the dependent variable to be normally distributed, to apply linear model.

Linear model requires (or the Linear Module solution is based on the assumption that the residuals are normally distributed). Let us understand this at a deeper level: ass to why the residuals are not normally distributed. Consider the case where the Y domain is (0,1) -- you may want to think about the residuals. What are the residuals? Y-predicted - Y-observed are the residuals. What are the possible residuals. Let us assume we engineer a model that transforms given X to Y such that Y is 0 or 1. I think about it as follows: (this is NOT rigorous but I am hoping it is helpful to understand) Some of the transformed Y will be correct. And others will be incorrect. that is a 1 where it ought to have been 0 and 0 where it ought to have been 1. So these residuals are (1,0) and a domain of (1,0) is not normally distributed. OLS also assumes homo-skedascity or  the absence of heterskedascity. the solution to Y = Ax+e the slope is same -- it is not trying to fit multiple line segments...if the variance of Y around each point is different,the Ys will be different and will result in multiple line segments ... for this we run BP test Breusch-Pagan test. Linear Models also assume Y(x) isindependent of Y(x-1) -- which is absence of auto-collinearity. That is Breusch-Godfrey test. Then finally If the dataset is Rank Deficient (multi-collinear -- features are not independent) then as a matter of practice we cannot even perform many basic Linear Algebra operations on rank deficient matrices -- therefore the OLS construct will crumble. Besides, if there two variables highly correlated, we do not need both variables -- our model does not require the correlated variables. So these are the assumptions for OLS. So please make a note of all assumptions of models and understand them. Before running a model, make sure assumptions are valid.
